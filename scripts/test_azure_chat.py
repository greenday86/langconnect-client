#!/usr/bin/env python3
"""Azure Chat(ë©€í‹°ì¿¼ë¦¬ìš© LLM) ì—°ë™ë§Œ í™•ì¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸. .envì˜ LLM_PROVIDER=azure ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""

import asyncio
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ .env ë¡œë“œ
root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(root))
os.chdir(root)

from dotenv import load_dotenv

load_dotenv()

LLM_PROVIDER = (os.getenv("LLM_PROVIDER") or "openai").lower()
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT", "")
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY", "")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01")


def get_llm():
    """MCP ì„œë²„ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
    if LLM_PROVIDER == "azure" and AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY:
        from langchain_openai import AzureChatOpenAI

        return AzureChatOpenAI(
            azure_endpoint=AZURE_OPENAI_ENDPOINT.rstrip("/"),
            api_key=AZURE_OPENAI_API_KEY,
            api_version=AZURE_OPENAI_API_VERSION,
            deployment_name=AZURE_OPENAI_DEPLOYMENT or "gpt-4o-mini",
            temperature=1,  # ì¼ë¶€ Azure ëª¨ë¸ì€ 0 ë¯¸ì§€ì›
        )
    from langchain_openai import ChatOpenAI

    key = os.getenv("OPENAI_API_KEY", "")
    if key:
        return ChatOpenAI(temperature=0, api_key=key)
    return None


async def main():
    print("ğŸ” Azure Chat(ë©€í‹°ì¿¼ë¦¬) ì—°ë™ í™•ì¸ ì¤‘...")
    print(f"   LLM_PROVIDER={LLM_PROVIDER}")
    print(f"   AZURE_OPENAI_ENDPOINT={AZURE_OPENAI_ENDPOINT or '(ë¹„ì–´ ìˆìŒ)'}")
    print(f"   AZURE_OPENAI_DEPLOYMENT={AZURE_OPENAI_DEPLOYMENT or '(ê¸°ë³¸ê°’ ì‚¬ìš©)'}")
    print()

    llm = get_llm()
    if llm is None:
        print("âŒ LLM ë¯¸ì„¤ì •: .envì— OPENAI_API_KEY ë˜ëŠ” (Azure) AZURE_OPENAI_* ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(1)

    # 1) ë‹¨ìˆœ í˜¸ì¶œ í…ŒìŠ¤íŠ¸
    print("1) ë‹¨ë¬¸ ì‘ë‹µ í…ŒìŠ¤íŠ¸...")
    try:
        msg = await llm.ainvoke("í•œ ë‹¨ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”: OK")
        text = msg.content if hasattr(msg, "content") else str(msg)
        print(f"   ì‘ë‹µ: {text.strip()}")
        print("   âœ… ë‹¨ë¬¸ í˜¸ì¶œ ì„±ê³µ")
    except Exception as e:
        print(f"   âŒ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # 2) ë©€í‹°ì¿¼ë¦¬ì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸
    print("\n2) ë©€í‹°ì¿¼ë¦¬ ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ (ì§ˆë¬¸ â†’ ì—¬ëŸ¬ ì¿¼ë¦¬ ìƒì„±)...")
    from langchain_core.prompts import PromptTemplate
    from langchain_core.output_parsers import BaseOutputParser

    class LineListOutputParser(BaseOutputParser[list[str]]):
        def parse(self, text: str) -> list[str]:
            lines = [line.strip() for line in text.strip().split("\n")]
            return [line for line in lines if line]

    prompt = PromptTemplate(
        input_variables=["question"],
        template="""You are an AI language model assistant. Your task is to generate 3 to 5 
different versions of the given user question to retrieve relevant documents from a vector 
database. By generating multiple perspectives on the user question, your goal is to help
the user overcome some of the limitations of the distance-based similarity search. 
Provide these alternative questions separated by newlines. Do not number them.
Original question: {question}""",
    )
    parser = LineListOutputParser()
    chain = prompt | llm | parser
    try:
        queries = await chain.ainvoke({"question": "RAGë€ ë¬´ì—‡ì¸ê°€?"})
        print(f"   ìƒì„±ëœ ì¿¼ë¦¬ ìˆ˜: {len(queries)}")
        for i, q in enumerate(queries, 1):
            print(f"   - {i}. {q[:60]}{'...' if len(q) > 60 else ''}")
        print("   âœ… ë©€í‹°ì¿¼ë¦¬ ìŠ¤íƒ€ì¼ í˜¸ì¶œ ì„±ê³µ")
    except Exception as e:
        print(f"   âŒ ì‹¤íŒ¨: {e}")
        sys.exit(1)

    print("\nâœ… Azure Chat(ë©€í‹°ì¿¼ë¦¬) ì—°ë™ì´ ì •ìƒì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    asyncio.run(main())
